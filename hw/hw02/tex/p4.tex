\begin{problem}
  Consider the problem of minimizing

  \[f(x_{1},x_{2},\cdots,x_{n}) = \sum_{i=1}^{n-1}[100(x^{2}_{i} - x_{i+1})^{2} +(x_{i} - 1)^{2}]\textrm{.}\]

  The global minimum is at $x^{âˆ—} = [1, 1,\cdots,1]\transpose$. Numerically solve this problem using nonlinear conjugate gradient algorithms:
  \begin{enumerate}
    \item FR (Algorithm 5.4)
    \item FR with restart based on Eq.~(5.52) in Nocedal and Wright
    \item PR based on Eq.~(5.44) in Nocedal and Wright
  \end{enumerate}

  \noindent
  and compare their performance. (In the numerical experiments you can set ${n = 100}$ or any number that is not too small. The initial condition can be chosen as a random vector, for example, $2 \cdot \text{rand}(n,1)$.)
\end{problem}

The gradient of $f$ is:

\[ \nabla f = \left\{
                \begin{array}{lc}
                  400x_i(x_{i}^{2} - x_{i+1}) + 2(x_i - 1)\text{,} & i = 1 \\
                  400x_i(x_{i}^{2} - x_{i+1}) + 2(x_i - 1) -200(x^{2}_{i-1} - x_{i})\text{,} & 1 < i < n \\
                  -200(x^{2}_{i-1} - x_{i})\text{,} & i = n \\
                \end{array}
              \right. \text{.} \]

\noindent
Define $\psi_i = x_i + \alpha p_i$.  Therefore, the derivative of $\phi$ is

\[ \phi'(\alpha) = \sum_{i=1}^{n-1} 200 (2p_i \psi_i - p_{i+1}) (\psi_{i}^{2} - \psi_{i+1})
                   + 2p_i(\psi_i - 1) \text{.} \]

\noindent
The parameters used for this experiment are specified in Table~\ref{tab:p04:ExperimentParams}.

\begin{table}[h]
  \centering
  \caption{Experiment parameters for problem~\#4}\label{tab:p04:ExperimentParams}
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Parameter} & \textbf{Value} \\
    \hline\hline
    $n$     & 100  \\\hline
    $c_1$   & 0.1  \\\hline
    $c_2$   & 0.45 \\\hline
    $v$     & 0.1  \\\hline
    $x_{0}$ & $2\cdot\text{rand}(n,1)$\\\hline
  \end{tabular}
\end{table}

\noindent
To calculate $\alpha$, I reused most of my implementation of inexact line search from homework~\#1.

Figures~\ref{fig:p04:FR},~\ref{fig:p04:FRwithRestart}, and~\ref{fig:p04:PR} show the performance of Fletcher-Reeves, Fletcher-Reeves with Restart, and Polak-Ribi\`{e}re respectively. In all experiments, the same random~$x_0$ was used to ensure consistency.  As expected, standard Fletcher-Reeves had the worst performance. While it slowly makes progress, even after 10,000~iterations standard Fletcher-Reeves did not come close to converging to the optimal solution.  Adding restart to Fletcher-Reeves improves the performance significantly, and the algorithm converged in about 900~iterations.  Polak-Ribi\`{e}re converged the fastest in approximately 375~iterations.

It is important to note that different random $x_0$ could drastically effect the number of iterations it took all the algorithms to converged.  In some rare cases, a different random start position caused Fletcher-Reeves with restart to converge the fastest.
\begin{figure}[p]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \input{pgfplots/p04_fr_plot}
    \caption{}\label{fig:p04:FR}
  \end{subfigure}
  ~
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \input{pgfplots/p04_fr_with_restart_plot}
    \caption{}\label{fig:p04:FRwithRestart}
  \end{subfigure}
  \caption{Problem~\#4 performance for Fletcher-Reeves without and with restart}
\end{figure}

\begin{figure}[p]
  \centering
  \input{pgfplots/p04_pr_plot}
  \caption{Problem~\#4 performance for the Polak-Ribi\`{e}re Method}\label{fig:p04:PR}
\end{figure}

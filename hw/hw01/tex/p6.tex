\begin{problem}
  Consider the problem of minimizing:

  \[ f(x_1,x_2) = (cx_{1} - 2)^4 + x_{2}^{2}(cx_{1}-2)^{2} + (x_{2} + 1)^{2} \textrm{,} \]

  \noindent
  where $c$ is a nonzero parameter.
\end{problem}

\begin{subproblem}
  Compute $\nabla f(x)$ and $\nabla f^{2}(x)$, and find the optimal solution.
\end{subproblem}

\noindent
\begin{aligncustom}
  \nabla f(x) &=  \begin{bmatrix}
                          \frac{\partial f(x)}{\partial x_1} \\
                          \frac{\partial f(x)}{\partial x_2}
                        \end{bmatrix} \\
                    &=  \boxed{
                          \begin{bmatrix}
                            4c (cx_{1} - 2)^{3} + 2cx_{2}^{2}(cx_{1}-2) \\
                            2x_2(cx_1-2)^2 + 2(x_2 + 1)
                          \end{bmatrix}
                        }
\end{aligncustom}

\begin{aligncustom}
  \nabla^{2} f(x) &=  \begin{bmatrix}
                        \frac{\partial^{2} f(x)}{\partial x_1^{2}} & \frac{\partial^{2} f(x)}{\partial x_1 \partial x_2} \\
                        \frac{\partial^{2} f(x)}{\partial x_2 \partial x_1} & \frac{\partial^{2} f(x)}{\partial x_{2}^{2}}
                      \end{bmatrix} \\
                  &=  \boxed{
                        \begin{bmatrix}
                          12c^2 (cx_{1} - 2)^{2} + 2c^2x_{2}^{2} & 4cx_2(cx_1 - 2) \\
                          4cx_2(cx_1 - 2) & 2(cx_1-2)^2 + 2
                        \end{bmatrix}
                      }\textrm{.}
\end{aligncustom}

The gradient,~$\nabla f(x)$, above was set equal to~$\vec{0}$ and the roots were solved for in Maple.  It returned root~$\boxed{\left(\frac{2}{c}, -1\right)\transpose}$.  To verify it is a local minimum, we need to substitute this into the Hessian.  Therefore,

\begin{aligncustom}
    \nabla^{2} f(x^{*}) &=  \begin{bmatrix}
                              2c^2 & 0 \\
                              0    & 2
                            \end{bmatrix}
\end{aligncustom}

\noindent
This matrix has eigenvalues $2c ^ 2$ and $2$.  Since $c \ne 0$ by definition, this matrix is positive definition for all valid values of $c$.

\begin{subproblem}
  Program the steepest descent method (using your program from Problem~\ref{prob:ProgAlg} to find the step length).  Use your program to numerically solve this problem under two cases: i) $c=1$, ii) $c=10$.  Compare the convergence in these two cases.
\end{subproblem}



\begin{table}[t]
  \centering
  \caption{Parameters used for the exact line search of problem~\#6}\label{tab:p6ParamValues}
  \begin{tabular}{|c|c|}
    \hline
    Parameter & Value \\\hline
    $c_1$ & 0.1 \\\hline
    $c_2$ & 0.9 \\\hline
    $tol$ & $10^{-16}$ \\\hline
    $x_0\transpose$ & $[10,10]$ \\\hline
  \end{tabular}
\end{table}

\noindent
Table~\ref{tab:p6ParamValues} lists the parameter values for this part of the problem. Figures~\ref{fig:p6C=1} and~\ref{fig:p6C=10} show the results of exact line search with the steepest descent method with for $c=1$ and $c=10$ respectively.  Note that for $c=1$, the result converged in only 7~iterations.  In contrast for $c=10$, convergence took 1,703~iterations.

\input{p6_plots}

\newpage 
\begin{subproblem}
  Using Theorem 3.4, explain why as $c$ increases the convergence of the steepest descent method deteriorates.
\end{subproblem}

For a matrix $A$ with sorted eigenvalues in ascending order $\lambda_1,\ldots,\lambda_n$, the condition number is defined as:

\[ \kappa(A) =\frac{\lambda_n}{\lambda_1} \text{.}\]

\noindent
In general, as $\kappa$ of the Hessian increases, the system has worse conditioning and takes more steps to converge.  For this problem $\kappa(\nabla^2 f) = c^2$.  Therefore, the convergence rates deteriorates quadratically with $c$.
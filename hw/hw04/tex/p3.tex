\begin{problem}\label{prob:03}%%
  Code Algorithm 7.5, and test it on the extended Rosenbrock function
  
  \[ f(x) = \sum_{i=1}^{n/2} \left[ \alpha(x_{2i} - x^2_{2i-1})^2 + (1-x_{2i-1})^2 \right] \text{.} \]
  
  where $\alpha$ is a parameter that you can vary (for example, 1 or 100). The solution is $\xopt=(1,1,...,1)\transpose$, $f^{*}=0$. Choose the starting point as $(-1,-1,...,-1)\transpose$. Observe the behavior of your program for various values of the memory parameter $m$.
\end{problem}

For even valued $n$, the gradient of $f$ is:

\[
\nabla f(x) = \left\{
                \begin{array}{cl}
                  -4\alpha x_{j}(x_{j+1} - x^2_{j}) - 2x_{j}(1-x_{j}) & j \text{ is odd}\\
                  2\alpha(x_j - x^{2}_{j-1}) & j \text{ is even}
                \end{array}
              \right.
\]

\noindent
where ${j \in \{1,\ldots,n\}}$.  Since LBFGS relies on line search, $\phi(\alpha) = f(x_k + \alpha_k p_k)$.  In addition, by the chain rule, 

\begin{aligncustom}
  \phi'(\alpha) &= \frac{\partial f(x_k+\alpha p_k)}{\partial \alpha} \\
                &= \frac{\partial f(x_k+\alpha p_k)}{\partial x_k} \cdot \frac{\partial x_k}{\partial \alpha} \\
                &= \nabla f(x_k + \alpha p_k) \cdot p_k \text{.}
\end{aligncustom}


